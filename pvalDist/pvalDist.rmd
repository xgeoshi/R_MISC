---
title: "Simulation of Revenue Distribution in R language. Process Reveal."
# mainfont: DejaVu Sans
output:
  pdf_document:
    toc: true
    df_print: kable
    # toc_depth: 1
    # number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

&nbsp;

## Randomly Generated Data Source as an Example.

```{r, source, cache=FALSE, echo=FALSE, message=FALSE}
require(data.table)
src <- data.frame(
  case = c("case1", "case2", "case3", "case4", "case5", "case6", "case7", "case8", "case9"),
  prob = c(0.85, 0.2345, 0.0555, 0.001, 0.35, 0.16, 0.68, 0.4, 0.12),
  reve = c(15000, 10000, 5000, 5000, 7000, 2000, 3000, 4000, 1000),
  stringsAsFactors = FALSE)
src <- as.data.table(src)
```

Simulated data source have size of `r ncol(src)` columns & `r nrow(src)` rows.

`r ncol(src)` columns:  

* `case id` - case id number
* `prob` - success probability of e.g. Opportunity
* `revenue` - revenue amount per Opportunity  

`r nrow(src)` rows:  

* `r nrow(src)` cases  

&nbsp;

View on Source content:

```{r, printsrc, echo=FALSE}
src
```

Code used to create source (mentioned above):
```{r, srccode}
src <- data.frame(
  case = c("case1", "case2", "case3", "case4", "case5", "case6", "case7", "case8", "case9"),
  prob = c(0.85, 0.2345, 0.0555, 0.001, 0.35, 0.16, 0.68, 0.4, 0.12),
  reve = c(15000, 10000, 5000, 5000, 7000, 2000, 3000, 4000, 1000),
  stringsAsFactors = FALSE)
p <- src$prob
```

\newpage



# Pipeline

In order to create revenue distribution (simulations) it's required to proceed with the following steps:  

a) Generate Probability Deviation Distribution for each of 7 case's probability values
b) Simulate binary outcome per case (success-failure) based on randomly chosen single prob value per distribution
c) Substitute success outcomes with related case's revenue
d) Sum success case values = Total Revenue (Simulated)
e) Repeat many times = Simulated Revenue Distribution

&nbsp;

## Generate Probability Deviation Distribution

Probability Deviation Distributions computed as a vector of certain length of
zeroes and ones (`"Bernulli trials"`) with given probability of success.
Example of how vector, say length = 25 with given 30% probability of success
would look like. Code & output:  

```{r}
set.seed(11)
bernulliTrial <- function(success_p, length) {
        sample(x       = c(0, 1),
               size    = length,
               replace = TRUE,
               prob    = c(1 - success_p, success_p))
        
}

bernulli_trial_1 <- bernulliTrial(success_p = 0.30, length = 25)
print(bernulli_trial_1)

```

  
&nbsp;

  
Mean Value of This Vector is what we can call a Simulation of Random Deviation (from 30% prob):
```{r}
mean(bernulli_trial_1)
```

&nbsp;


If we run same function again we would get slightly different result since function is randomized over mean:
```{r, echo=FALSE}
set.seed(13)
```

```{r}
bernulli_trial_2 <- bernulliTrial(success_p = 0.30, length = 25)
print(bernulli_trial_2)
```

&nbsp;

```{r}
mean(bernulli_trial_2)
```

&nbsp;

Repeating this process = 10K times will be sufficient for our purposes.

But before proceed we need to make sure length of the bernulli vector is enough
to meet 10 > success-failure binomial model conditions required for normal
distribution.

Check sufficiency for length 25 given 30% probability success as follows:
```{r}
t(c("25*0.3" = 0.3 * 25, "25*0.7" = (1 - 0.3) * 25))
```

7.5 is less then 10 necessary success so we need to set longer vector using equation:

```{r}
print(length_for_30p <- 10 / min(c(1 - 0.3, 0.3)))
```

&nbsp;

Let's check the sufficiency of the new vetor length > 10 success condition:
```{r}
t(c("0.3" = length_for_30p * 0.3, "0.7" = length_for_30p * (1 - 0.3)))
```

&nbsp;

Since we need to put all cases in the similar condition while simulation, we have
to take the longest suitable vector among all cases and apply it for every case:

```{r tail}
# Minimum Length (for 10 Success-Failures binomial condition) ----
print(min.tail <- min(p, 1 - p)) # min value of all range of both heads and tails
```

&nbsp;

Proportion for length size (x to be at least 10):
```{r len}
print(min.len  <- ceiling(10 / min.tail)) # min length to get 10 binom successes
```

&nbsp;

Minimum length of the vector is set as a function argument to compute 10K mean
bernulli means per each case of source data.

Upon that I'll creat a function to use it as input for another functions:

```{r}
# Minimum Length (for 10 Success-Failures binomial condition) ----
pMinLen <- function(p) {
        
        # check for probability condition
        stopifnot(p >= 0 & p <= 1)
        
        # indexing zeroes an ones (0% & 100% probability)
        p01 <- which(p == 0 | p == 1)
        
        # return NA if input consist of only zeroes and/or ones
        if (length(p01) == length(p)) return(NA)
        
        # condition in case some zeroes and ones present
        if (length(p01) > 0) {
                # vector without zeroes & ones + meassge
                non01_p <- p[-p01]
                message("<pMinLen>: ", length(p01),
                        " zeroes & ones are removed / total ", length(p), " p")
        }
        
        # condition if there are no zeroes & ones
        if (length(p01) == 0) non01_p <- p
        
        # min value of all range of both heads and tails of non zero & ones
        min.p <- min(non01_p, 1 - non01_p)
        # proportion to evaluate min length to get 10 binom successes
        min.len  <- ceiling(10 / min.p)
        
        stopifnot(length(min.len) == 1L && min.len > 0)
        message(min.p, " minimum length for bernulli success-failure cond: ",
                min.len)
        return(min.len)
}
```

Let's test if the result areidentical:
```{r}
test.tail <- pMinLen(p = p)
print(test.tail)
print(min.tail == test.tail)
```



Code:

```{r}
# Return vector of Zero & Ones with given success (1) probability & given length
bernulli <- function(success_p, length) {
        
        stopifnot(length(success_p) == 1L)
        if (success_p == 0 | success_p == 1) return(rep(success_p, length))
        
        sample(x       = c(0, 1),
               size    = length,
               replace = TRUE,
               prob    = c(1 - success_p, success_p))
}

# Function to create deviation distribution of given length for each p
# list of binomial means for given P vector - p distibutions of given length
# list of binomial means for given P vector - p distibutions of given length
bernulliMeansDist <- function(pvec, rep = 10000, each, export) {
        require(foreach)
  
        mlen <- pMinLen(pvec)
        # stopifnot(mlen > 0)
        
        nthMean <- function(n) {

                p <- pvec[n] # single nth probability value (vectorised)

                if (p == 0) p.dist <- numeric(length = rep)
                if (p == 1) p.dist <- rep(1, length = rep)

                if (p > 0 & p < 1) {
                        p.dist <- replicate(rep,
                                            mean(bernulli(success_p = p,
                                                          length = mlen)))
                }

                return(p.dist)
        }
        
        if (each) {
                dist.ls <- foreach(n = seq(length(pvec)),
                                   .export = export) %dopar% nthMean(n = n)
                
        } else {
                
                dist.ls <- lapply(seq(length(pvec)), function(n) nthMean(n))
        }
        
        
        stopifnot(length(unique(vapply(dist.ls, length, numeric(1)))) == 1L)
        stopifnot(length(dist.ls) == length(pvec))
        
        names(dist.ls) <- paste0(seq_along(pvec), "_", pvec)
        
        return(dist.ls)
}

```

&nbsp;

Deviations Distributions:
```{r, cache=FALSE}
pmeandist <- bernulliMeansDist(pvec = src$prob, rep = 10000, each = FALSE, export = "mtcars")
str(pmeandist)

```

&nbsp;

Let's Plot histograms of Probability Deviations for every case:

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
require(ggplot2)
require(data.table)
require(forcats)
plot.data <- list()

for (r in seq(length(pmeandist))) {
        plot.data[[r]] <- data.table(p = pmeandist[[r]], cat = names(pmeandist[r]))
}

ggplot(data = rbindlist(plot.data), aes(x = p)) +
        geom_histogram() +
        facet_wrap(~ as_factor(cat), scales = "free_x")
```

\newpage

## Simulate binary outcome per case (success-failure)

Now we need to simulate binary oucomes upon simulated probabilities distrbutions
for every case x 10000 times.
Code:

```{r}
# VECTOR OF SIMULATED BINARY OUTCOMES UPON SINGLE RANDOM SUCCESS PROBABILITY
# FROM EACH DISTRIBUTION OF PROBABILITIES 
binarySim <- function(distr.p) {
        
        # take by 1 random value from each distribution (for each probability)
        rand.each1 <- vapply(distr.p, function(x) sample(x, 1), numeric(1))
        
        # 1 sample from each dist as "success probability" for binary simulation
        bin.sim <- vapply(rand.each1, function(b) {
                sample(x       = c(0, 1),
                       size    = 1,
                       replace = TRUE,
                       prob    = c(1 - b, b))
                }, 
                numeric(1))
        
        stopifnot(length(distr.p) == length(bin.sim))
        return(bin.sim)
}

```

&nbsp;

Example of outcome after repeated running for x10 times:

```{r, cache=TRUE}
set.seed(2)
replicate(10, binarySim(pmeandist))
```

## Revnues Distribution

Finaly we add everything up.
We use 10000 simulated binary outcomes per each case, combine it with 
related Opportunity ID revenue and sum all values to have simulation of Total Revnue.
And Repeat x10K Times.

Code:

```{r, cache=TRUE}

# Apply each binary outcome to related revenue
bernulliTimesVal <- function(values, distr) {
        sum(values * binarySim(distr.p = distr))
}

# SIMULATED TOTAL REVENUE DISTRIBUTION
pvalDist <- function(pvec,
                     valvec,
                     rep = 10000,
                     parallel,
                     export = c("bernulliTimesVal", "valvec",
                                           "pmeandist", "binarySim", "actual")) {
        
        repl.args <- list(n = rep)
        
        if (parallel) {
                require(future)
                require(doFuture)
                registerDoFuture()
                plan(multisession)
                
                source("parallelRep.R", local = TRUE)
                message("<pvalDist> parallel")
                repl.args[["each"]] <- TRUE
                repl.args[["export"]] <- export
                replX <- parallelRep
                
        } else {
                replX <- replicate
        }
        
        # Creating list of berbulli means distribution
        pmeandist <- bernulliMeansDist(pvec = pvec, each = parallel,
                                       export = export)
        stopifnot(identical(length(pmeandist), length(valvec), length(pvec)))
        
        # Adding argument
        repl.args[["expr"]] <- quote(bernulliTimesVal(values = valvec,
                                                      distr = pmeandist))
        
        # revenue simulations
        rsim <- do.call(replX, repl.args)
        
        stopifnot(length(rsim) == rep)
        return(rsim)
}

```


```{r, cache=TRUE, message=FALSE}
set.seed(1)
simdist <- pvalDist(pvec = src$prob, valvec = src$reve, rep = 10000,
                    parallel = TRUE)
```


Output head:


```{r}
str(simdist)
```


Total Revenue Distribution Simulation Histogram:


```{r, plotartif, message=FALSE, echo=FALSE}
require(ggplot2)
require(scales)
plotdist <- function(simdist) {
        ggplot(data = as.data.table(simdist)) +
          geom_histogram(aes(x = simdist, y = ..density..)) +
          scale_x_continuous("revenue", labels = scales::dollar_format()) +
          scale_y_continuous("density") +
          geom_vline(aes(xintercept = mean(simdist)), col = 'black', size = 1) +
          geom_text(aes(label = paste0("mean: ",
                                       scales::dollar(mean(simdist))),
                        y = 0, x = mean(simdist)),
                    vjust = -0.4, hjust = -0.05, col = 'white', size = 5)
}

plotdist(simdist = simdist)
```


Mathematical Expectation of Example Dataset is exactly around simulated mean!:


```{r, mathexpnum}
sum(src$reve * src$p)
```


Those we can conclude that is this algorythim doesn't catch mean on actual data
it means that probabilities are far from perfect.

\newpage

## Real Data


Now we'll apply this approach to real data.


We'll use data from snapshot of 2018 October 1 which is closed for today (so we could evaluate simulation).


\scriptsize


Code for download:


```{r, message = FALSE, cache=TRUE}
source("_000sqlQueryDB.R")
require(data.table)
require(dplyr)

actual <- sqlQueryDB("SELECT [Opportunity ID], [Winning_Probability], [Probability %],
[Probability_Of_Success], [Predicted Probability], [wonnow], [Amount USD], [AmountUSDnow]
      FROM (SELECT
            pre.*,
            now.[IsClosed] AS close2,
            now.[Stage Name] AS stg2,
            now.[SnapshotDate] AS snap2,
            now.[IsWon] AS wonnow,
            now.[Amount USD] AS AmountUSDnow,
            now.[Accurate revenue estimate Current FY] as CurrentFYnow,
            now.[Opportunity Type] AS otypenow
  		FROM (SELECT *
FROM datamart_tableau.dbo.vCRM_SF_MasterDataSource_Snapshot
WHERE (CAST(SnapshotDate AS date) = CAST('2018-10-01' AS date)) AND (SnapshotType = 'WEEKLY') AND
						[IsClosed] = 0 AND
						[Stage Name] NOT IN ('Agreement/Closing', 'Work at risk', 'Won/Signed')
						) as pre
  INNER JOIN
	  (SELECT
		[Opportunity ID],
		[IsClosed],
		[Stage Name],
		[SnapshotDate],
		[IsWon],
		[Accurate revenue estimate Current FY],
		[Amount USD],
		[Opportunity Type]
		FROM [datamart_tableau].[dbo].[vCRM_SF_MasterDataSource_Snapshot] AS targ
		WHERE
		[IsClosed] IN (1) AND [SnapshotDate] = (SELECT max([SnapshotDate]) from
		[datamart_tableau].[dbo].[vCRM_SF_MasterDataSource_Snapshot])) as now
 ON pre.[Opportunity ID] = now.[Opportunity ID]) AS final")

setDT(actual)
glimpse(actual)

```


\normalsize


As we can see there are several kind of probabilities non empty is
`Probability %` only so we model the revenue using it:



```{r, message=FALSE, warning=FALSE}
require(skimr)
prp <- actual$`Probability %`
skim(prp) %>% pander()
```



```{r, cache=TRUE}
require(skimr)
sim_pp <- pvalDist(pvec = actual$`Probability %`,
                   valvec = actual$`Amount USD`,
                   rep = 10000,
                   parallel = TRUE)

```


```{r, message=FALSE}
plotdist(sim_pp)
```



```{r, warning=FALSE}
skim(sim_pp) %>% pander() # all simulations statistics
```



now let's try to simulate revenues using `Winning_Probability` variable.
Since there lot's of missing values we'll replace missing with `Probability %`

Without imputation (423 missing values):


```{r, warning=FALSE}
wpr <- actual$Winning_Probability
skim(wpr) %>% pander()
```



```{r}
require(dplyr)
ful.win.prob <- case_when(is.na(actual$Winning_Probability) ~ actual$`Probability %`,
                          TRUE ~ actual$Winning_Probabilit)

```


```{r, warning=FALSE}
skim(ful.win.prob) %>% pander() # inspecting for non missing probabilities vector
```


simulating revenues using new probabilities source:


```{r, cache=TRUE}
sim_win_prob <- pvalDist(pvec = ful.win.prob,
                         valvec = actual$`Amount USD`,
                         rep = 10000,
                         parallel = TRUE)

```



```{r, message=FALSE}
plotdist(sim_win_prob)
```

Lastly we'll simulate probabilities using Machine Learning for if we were in
1 October 2018:


```{r, message=FALSE, cache=FALSE}
source("../../R_OPPO_PROB/_oppoJoinProbs.R", local = TRUE)
nowwd <- getwd()
setwd("../../R_OPPO_PROB")
ml_prob <- oppoJoinProbs(given_date = "2018-10-01")
head(ml_prob_slim <- ml_prob[, c("Opportunity ID", "Predicted Probability")])
pprob <- ml_prob_slim$`Predicted Probability`
setwd(nowwd)
```


```{r, warning=FALSE}
skim(pprob) %>% pander()
```


Now we'll join siulate probabilities for given date to our `actual` dataset:


```{r}
join_ml <- left_join(actual, ml_prob_slim, by = "Opportunity ID")
head(join_ml)

```


now let's see if there are any missing..


```{r, warning=FALSE}
pprob.y <- join_ml$`Predicted Probability.y`
skim(pprob.y) %>% pander()
```


No, 0 missings. So we can start simulation.


```{r, cache = TRUE}
sim_ml_prob <- pvalDist(pvec = join_ml$`Predicted Probability.y`,
                         valvec = join_ml$`Amount USD`,
                         rep = 10000,
                         parallel = TRUE)
```


```{r, message = FALSE}
plotdist(sim_ml_prob)
```


Concluson.
We haved proved upon simulation that our Probabilities model has highest fidelity.
As We can see based on 3 probabilities types, simulation mean of the one based
on Machine learning are much closer to the acutal value. Which is:


```{r}
require(scales)
scales::dollar(actual[wonnow == "1", sum(`Amount USD`)])
```



Or could be in case Amounts per Opportinity won't change:

```{r}
scales::dollar(actual[wonnow == "1", sum(AmountUSDnow)])
```




So the next step is to add Coefficient that would simulate Changes in
Opportunity amount on random... To be continued...