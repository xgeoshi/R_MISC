---
title: "Simulation of Revenue Distribution in R language. Process Reveal."
# mainfont: DejaVu Sans
output:
  pdf_document:
    toc: true
    df_print: kable
    # toc_depth: 1
    # number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

&nbsp;

## Intro.
Description of the algorithm simulating total Revenue (Revenue Distribution)
upon given Opportunities, their amounts and the prediction of the probability
of winning.  

In the heart of the simulation is the sum of outcomes of Bernoulli
trials (or binomial trial) which is a random experiment with exactly two
possible outcomes per Opportunity case, "success" or "failure", where the
probability of success is the near the same (every time the experiment is
conducted within hundreds thousand iteration.


## Generate small random dataset to start with.
### Why
In order to make it easier to understand how the full cycle of simulation works
and what results it leads to, I apply it to artificially created small dataset:

### Small dataset

<!-- R code for dataset creation:   -->

```{r, source, echo=FALSE, message=FALSE}
require(data.table)
src <- data.frame(
  case = c("case1", "case2", "case3", "case4", "case5", "case6", "case7", "case8", "case9"),
  prob = c(0.85, 0.2345, 0.0555, 0.001, 0.35, 0.16, 0.68, 0.4, 0.12),
  revenue = c(15000, 10000, 5000, 5000, 7000, 2000, 3000, 4000, 1000),
  stringsAsFactors = FALSE)
src <- as.data.table(src)
```

Simulated data source have size of `r ncol(src)` columns & `r nrow(src)` rows.

`r ncol(src)` columns:  

* `case id` - simulated case id number
* `prob` - success probability of e.g. Opportunity
* `revenue` - revenue amount per Opportunity  

`r nrow(src)` rows:  

* `r nrow(src)` cases  

<!-- &nbsp; -->

Let's have a look on a small dataset in table format:

```{r, printsrc, echo=FALSE}
src
```

R Code used to create source (mentioned above):
```{r, srccode}
src <- data.frame(
  case = c("case1", "case2", "case3", "case4", "case5", "case6", "case7", "case8", "case9"),
  prob = c(0.85, 0.2345, 0.0555, 0.001, 0.35, 0.16, 0.68, 0.4, 0.12),
  revenue = c(15000, 10000, 5000, 5000, 7000, 2000, 3000, 4000, 1000),
  stringsAsFactors = FALSE)

p <- src$prob # Storing Probabilities as separate vector "p"
```

<!-- \newpage -->


## Pipeline:  

In order to create revenue distribution (simulations) we will go through the
following steps:  

a) Generate Probability Deviation Distribution for each of `r nrow(src)` case's
probability values
b) Simulate binary outcomes per case (success-failure) based on randomly chosen
single prob value per distribution from part a.
c) Sum binary outcomes multiplied by related contract Value. x 1Mln = Simulated
Revenue Distribution


&nbsp;


### Pipeline: part 1. Generate Probability Deviation Distribution

Probability Deviation Distributions computed as a vector of certain length of
zeroes and ones (`"Bernulli trials"`) with given probability of success. This
is required to simulate variablity in probability prediction due to natural
random chance.  

Let's have a look on a vector, say length = 25 with given 30% probability of
success.  

Code for a function:  

```{r}
set.seed(11)
bernulli <- function(success_p, length) {
        
        stopifnot(length(success_p) == 1L)
        if (success_p == 0 | success_p == 1) return(rep(success_p, length))
        
        sample(x       = c(0, 1),
               size    = length,
               replace = TRUE,
               prob    = c(1 - success_p, success_p))
}
```


&nbsp;


Function Evaluation Output:

```{r}
set.seed(22)
print(bernulli_trial_1 <- bernulli(success_p = 0.30, length = 25))
```

  
&nbsp;

  
As we can see vector's mean value is near given 0.3 probability since it's
randomly generated:
```{r}
mean(bernulli_trial_1)
```


&nbsp;


If we run same function again we would get slightly different result,
since function is randomized over the mean and it should follow
normal distribution if all the required conditions are met:

```{r}
set.seed(14)
print(bernulli_trial_2 <- bernulli(success_p = 0.30, length = 25))
```


&nbsp;


Check second vector mean value again:
```{r}
mean(bernulli_trial_2)
```


&nbsp;


Repeating this process = 10K times will be sufficient for our purposes of
modeling normal distribution from the given value for each probability value.

But before proceeding with that we need to make sure the length of the bernulli
vector is sufficient to meet `10 > success-failure` binomial model conditions
required for near normal distribution.

We perform a check up of the simulated vector of length 25 to be sufficient in
case of given 30% probability success, as follows:

```{r}
t(c("25 * 0.3" = 0.3 * 25, "25 * 0.7" = (1 - 0.3) * 25))
```


&nbsp;


7.5 is less then > 10 necessary condition of successes so we need to set
sufficient longer vector using the following equation:

```{r}
print(length_for_30p <- 10 / min(c(1 - 0.3, 0.3)))
```


&nbsp;


Let's check the sufficiency of the new vetor length > 10 success condition:
```{r}
t(c("0.3" = length_for_30p * 0.3, "0.7" = length_for_30p * (1 - 0.3)))
```


&nbsp;


Done.
Moreover, Since It's an experiment we need to put all the Opportunities in the
identical conditions while simulation, so we would have to take the longest
suitable vector among all cases and apply it for every case.  

Identifying the longest of sufficient vectors as a proportion for length size
(to be at least 10):

```{r tail}
# Minimum Length (for 10 Success-Failures binomial condition) ----
min.tail <- min(p, 1 - p) # min value of all range of both heads and tails
print(min.len  <- ceiling(10 / min.tail)) # min length to get 10 binom successes
```


&nbsp;


Below is the function that take vector of all probabilities as an argument and
define required length of the vector for probabilities means simulation.   


Code:

```{r}
# Minimum Length (for 10 Success-Failures binomial condition) ----
pMinLen <- function(p) {
        
        # check for probability condition
        stopifnot(p >= 0 & p <= 1)
        
        # indexing zeroes an ones (0% & 100% probability)
        p01 <- which(p == 0 | p == 1)
        
        # return NA if input consist of only zeroes and/or ones
        if (length(p01) == length(p)) return(NA)
        
        # condition in case some zeroes and ones present
        if (length(p01) > 0) {
                # vector without zeroes & ones + meassge
                non01_p <- p[-p01]
                message("<pMinLen>: ", length(p01),
                        " zeroes & ones are removed / total ", length(p), " p")
        }
        
        # condition if there are no zeroes & ones
        if (length(p01) == 0) non01_p <- p
        
        # min value of all range of both heads and tails of non zero & ones
        min.p <- min(non01_p, 1 - non01_p)
        # proportion to evaluate min length to get 10 binom successes
        min.len  <- ceiling(10 / min.p)
        
        stopifnot(length(min.len) == 1L && min.len > 0)
        message(min.p, " minimum length for bernulli success-failure cond: ",
                min.len)
        return(min.len)
}
```


Check the contents of the vector `p`:
```{r}
print(p)
```


&nbsp;


Applying vector `p` as an argument to the function written above:

```{r}
print(min.length <- pMinLen(p = p))
```


Now we need to create functions that creates distribustion of length given above
for each probability we have and repeat the process given times.  
10K would be enough for our purposes.  

Code:

```{r}
# Return vector of Zero & Ones with given success (1) probability & given length
bernulli <- function(success_p, length) {
        
        stopifnot(length(success_p) == 1L)
        if (success_p == 0 | success_p == 1) return(rep(success_p, length))
        
        sample(x       = c(0, 1),
               size    = length,
               replace = TRUE,
               prob    = c(1 - success_p, success_p))
}

# Function to create deviation distribution of given length for each p
# list of binomial means for given P vector - p distibutions of given length
# list of binomial means for given P vector - p distibutions of given length
bernulliMeansDist <- function(pvec, rep = 10000, each, export) {
        require(foreach)
  
        mlen <- pMinLen(pvec)
        # stopifnot(mlen > 0)
        
        nthMean <- function(n) {

                p <- pvec[n] # single nth probability value (vectorised)

                if (p == 0) p.dist <- numeric(length = rep)
                if (p == 1) p.dist <- rep(1, length = rep)

                if (p > 0 & p < 1) {
                        p.dist <- replicate(rep,
                                            mean(bernulli(success_p = p,
                                                          length = mlen)))
                }

                return(p.dist)
        }
        
        if (each) {
                dist.ls <- foreach(n = seq(length(pvec)),
                                   .export = export) %dopar% nthMean(n = n)
                
        } else {
                
                dist.ls <- lapply(seq(length(pvec)), function(n) nthMean(n))
        }
        
        
        stopifnot(length(unique(vapply(dist.ls, length, numeric(1)))) == 1L)
        stopifnot(length(dist.ls) == length(pvec))
        
        names(dist.ls) <- paste0("case", seq_along(pvec), "_p", pvec)
        
        return(dist.ls)
}

```


&nbsp;


Executing functions mentioned above to get probabilities random deviation
distributions and viewing heading of the output list of values:

```{r, pmeansdist, message = FALSE, cache=FALSE}
pmeandist <- bernulliMeansDist(pvec = src$prob,
                               rep = 10000,
                               each = FALSE,
                               export = "mtcars")
str(pmeandist)

```


&nbsp;


Let's Plot histograms of Probability Deviations for every case we had:

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=FALSE}
require(ggplot2)
require(data.table)
require(forcats)
plot.data <- list()

for (r in seq(length(pmeandist))) {
        plot.data[[r]] <- data.table(p = pmeandist[[r]],
                                     cat = names(pmeandist[r]))
}

ggplot(data = rbindlist(plot.data), aes(x = p)) +
        geom_histogram() +
        facet_wrap(~ as_factor(cat), scales = "free_x")
```

\newpage

### Pipeline: part 2. Simulate binary outcome per case (success-failure)

Now we need to simulate binary oucomes upon simulated probabilities distrbutions.

Code:

```{r}
# VECTOR OF SIMULATED BINARY OUTCOMES UPON SINGLE RANDOM SUCCESS PROBABILITY
# FROM EACH DISTRIBUTION OF PROBABILITIES 
binarySim <- function(distr.p) {
        
        # take by 1 random value from each distribution (for each probability)
        rand.each1 <- vapply(distr.p, function(x) sample(x, 1), numeric(1))
        
        # 1 sample from each dist as "success probability" for binary simulation
        bin.sim <- vapply(rand.each1, function(b) {
                sample(x       = c(0, 1),
                       size    = 1,
                       replace = TRUE,
                       prob    = c(1 - b, b))
                }, 
                numeric(1))
        
        stopifnot(length(distr.p) == length(bin.sim))
        return(bin.sim)
}

```


&nbsp;


Example of outcome after repeated running for x10 times:

```{r, cache=FALSE}
set.seed(2)
replicate(10, binarySim(pmeandist))
```


&nbsp;


### Pipeline: part 3. Binary outcomes multiply by contract Value.

Finaly we add everything up.
We use 10000 simulated binary outcomes per each case, combine it with 
related Opportunity ID revenue and sum all values to have simulation of Total Revnue.
And Repeat x10K Times.

Code:

```{r, cache=TRUE}

# Apply each binary outcome to related revenue
bernulliTimesVal <- function(values, distr, coefs = NULL) {

        bin.sim <- binarySim(distr.p = distr)
        
        if (!is.null(coefs) && any(bin.sim == 1)) {
                bin.ones.index <- which(bin.sim == 1)
                len.ones <- length(values[bin.ones.index])
                
                smp.coef <- sample(coefs, len.ones, replace = TRUE)
                v <- values[bin.ones.index]
                stopifnot(length(v) == length(smp.coef))
                values[bin.ones.index] <- v - (v * smp.coef)
        }
        
        return(sum(values * bin.sim))

}


# SIMULATED TOTAL REVENUE DISTRIBUTION
pvalDist <- function(pvec,
                     valvec,
                     rep = 10000,
                     parallel,
                     export = c("bernulliTimesVal", "valvec",
                                "pmeandist", "binarySim", "actual", "coefs"),
                     coefs = NULL) {
        
        repl.args <- list(n = rep)
        
        if (parallel) {
                require(future)
                require(doFuture)
                registerDoFuture()
                plan(multisession)
                
                source("parallelRep.R", local = TRUE)
                message("<pvalDist> parallel")
                repl.args[["each"]] <- TRUE
                repl.args[["export"]] <- export
                replX <- parallelRep
                
        } else {
                replX <- replicate
        }
        
        # Creating list of berbulli means distribution
        pmeandist <- bernulliMeansDist(pvec = pvec, each = parallel,
                                       export = export)
        stopifnot(identical(length(pmeandist), length(valvec), length(pvec)))
        
        # Adding argument
        repl.args[["expr"]] <- quote(bernulliTimesVal(values = valvec,
                                                      distr = pmeandist,
                                                      coefs = coefs))
        
        # revenue simulations
        rsim <- do.call(replX, repl.args)
        
        stopifnot(length(rsim) == rep)
        return(rsim)
}

```


```{r, cache=TRUE, message=FALSE, cache=FALSE}
set.seed(1)
simdist <- pvalDist(pvec = src$prob, valvec = src$revenue, rep = 10000,
                    parallel = TRUE)
```

Output head:


```{r}
str(simdist)
```


&nbsp;


Total Revenue Distribution Simulation Histogram:

```{r, plotartif, message=FALSE, echo=FALSE}
require(ggplot2)
require(scales)

plotdist <- function(simdist) {
        ggplot(data = as.data.table(simdist)) +
          geom_histogram(aes(x = simdist, y = ..density..)) +
          scale_x_continuous("revenue", labels = scales::dollar_format(),
                             breaks = scales::pretty_breaks(20)) +
          scale_y_continuous("density") +
          geom_vline(aes(xintercept = mean(simdist)), col = 'black', size = 1) +
          geom_text(aes(label = paste0("mean: ",
                                       scales::dollar(mean(simdist))),
                        y = 0, x = mean(simdist)),
                    vjust = -0.4, hjust = -0.05, col = 'white', size = 5) +
        theme(axis.text.x = element_text(angle = 90, hjust = 1))
}
```


```{r}
plotdist(simdist = simdist)
```

### Conclusion on a small dataset.

Mathematical Expectation is exactly around mean of simulated distribution so
we may assume that simulation works as expected and the real value might be out
of the simulated range only in case of poor probabilities estimation.


```{r, mathexpnum}
sum(src$reve * src$p)
```

\newpage

## Real Data

Now we'll apply identical copy-paste approach to real data and add some extra
features on top:
a) Generate Probability Deviation Distribution for each of case's
probability values
b) Simulate binary outcomes per case (success-failure) based on randomly chosen
single prob value per distribution from part a.
c) Sum binary outcomes multiplied by related contract Value and changes
coefficient. x 1Mln = Simulated


We'll use Opportunities snapshot as of 2018 October 1 and filter those are 
closed (won or lost) as for now. Thus we know the outcome in advance for
evaluating simulation accuracy.


\scriptsize


Code for download:


```{r, message = FALSE, cache=TRUE}
source("_000sqlQueryDB.R")
require(data.table)
require(dplyr)

actual <- sqlQueryDB("SELECT [Opportunity ID], [Winning_Probability],
[Probability %], [Probability_Of_Success], [Predicted Probability], [wonnow],
[Amount USD], [AmountUSDnow]
      FROM (SELECT
            pre.*,
            now.[IsClosed] AS close2,
            now.[Stage Name] AS stg2,
            now.[SnapshotDate] AS snap2,
            now.[IsWon] AS wonnow,
            now.[Amount USD] AS AmountUSDnow,
            now.[Accurate revenue estimate Current FY] as CurrentFYnow,
            now.[Opportunity Type] AS otypenow
  		FROM (SELECT *
FROM datamart_tableau.dbo.vCRM_SF_MasterDataSource_Snapshot
WHERE (CAST(SnapshotDate AS date) = CAST('2018-10-01' AS date)) AND
(SnapshotType = 'WEEKLY') AND
						[IsClosed] = 0 AND
						[Stage Name] NOT IN ('Agreement/Closing',
						'Work at risk', 'Won/Signed')
						) as pre
  INNER JOIN
	  (SELECT
		[Opportunity ID],
		[IsClosed],
		[Stage Name],
		[SnapshotDate],
		[IsWon],
		[Accurate revenue estimate Current FY],
		[Amount USD],
		[Opportunity Type]
		FROM
		[datamart_tableau].[dbo].[vCRM_SF_MasterDataSource_Snapshot] AS targ
		WHERE
		[IsClosed] IN (1) AND [SnapshotDate] = (SELECT max([SnapshotDate]) from
		[datamart_tableau].[dbo].[vCRM_SF_MasterDataSource_Snapshot])) as now
 ON pre.[Opportunity ID] = now.[Opportunity ID]) AS final")

setDT(actual)
glimpse(actual)

```


\normalsize


&nbsp;


## 3 Probabilities Type to evaluate using simulation

There will be 3 Probailities type:
- `Machine Learning` ("hand made")
- `Probability %` - project managmet expectations
- `Winning_Probability` - SalesForce prediction


Let's take a look on each of them:


&nbsp;


### Probability Type 1: `Probability %`

As we can see there are several kind of probabilities non empty is
`Probability %` only so we model the revenue using it:


```{r, message=FALSE, warning=FALSE}
require(skimr)
prp <- actual$`Probability %`
skim(prp) %>% pander()
```


```{r, cache=TRUE}
require(skimr)
sim_pp <- pvalDist(pvec = actual$`Probability %`,
                   valvec = actual$`Amount USD`,
                   rep = 10000,
                   parallel = TRUE)

```


```{r, message=FALSE}
plotdist(sim_pp)
```



```{r, warning=FALSE}
skim(sim_pp) %>% pander() # all simulations statistics
```


&nbsp;


### Probability Type 2: `Winning_Probability`


now let's try to simulate revenues using `Winning_Probability` variable.
Since there lot's of missing values we'll replace missing with `Probability %`

Without imputation (423 missing values):


```{r, warning=FALSE}
wpr <- actual$Winning_Probability
skim(wpr) %>% pander()
```



```{r}
require(dplyr)
ful.win.prob <- case_when(is.na(actual$Winning_Probability) ~ actual$`Probability %`,
                          TRUE ~ actual$Winning_Probabilit)

```


```{r, warning=FALSE}
skim(ful.win.prob) %>% pander() # inspecting for non missing probabilities vector
```


&nbsp;


simulating revenues using new probabilities source:


```{r, cache=TRUE}
sim_win_prob <- pvalDist(pvec = ful.win.prob,
                         valvec = actual$`Amount USD`,
                         rep = 10000,
                         parallel = TRUE)

```



```{r, message=FALSE}
plotdist(sim_win_prob)
```


&nbsp;


### Probability Type 3: `Predicted Probability` (Machine Learning)


Lastly we'll simulate probabilities using Machine Learning for if we were in
1 October 2018:


```{r, message=FALSE, cache=FALSE}
source("../../R_OPPO_PROB/_oppoJoinProbs.R", local = TRUE)
nowwd <- getwd()
setwd("../../R_OPPO_PROB")
ml_prob <- oppoJoinProbs(given_date = "2018-10-01")
setwd(nowwd)
```


```{r}
head(ml_prob_slim <- ml_prob[, c("Opportunity ID", "Predicted Probability")])
pprob <- ml_prob_slim$`Predicted Probability`
```


```{r, warning=FALSE}
skim(pprob) %>% pander()
```


&nbsp;


Now we'll join siulate probabilities for given date to our `actual` dataset:


```{r}
join_ml <- left_join(actual, ml_prob_slim, by = "Opportunity ID")
head(join_ml)

```


&nbsp;


now let's see if there are any missing..


```{r, warning=FALSE}
pprob.y <- join_ml$`Predicted Probability.y`
skim(pprob.y) %>% pander()
```


&nbsp;


No, 0 missings. So we can start simulation.


```{r, cache = TRUE}
sim_ml_prob <- pvalDist(pvec = join_ml$`Predicted Probability.y`,
                         valvec = join_ml$`Amount USD`,
                         rep = 10000,
                         parallel = TRUE)
```


```{r, message = FALSE}
plotdist(sim_ml_prob)
```


&nbsp;


### Concluson.
We haved proved upon simulation that existing Machine Learning Probabilities
model has highest fidelity. As We can see based on 3 probabilities types,
simulation mean of the one based on Machine learning are much closer to the
acutal value. Which is:


```{r}
require(scales)
scales::dollar(actual[wonnow == "1", sum(`Amount USD`)])
```


&nbsp;


Or could be in case Amounts per Opportinity won't change:

```{r}
scales::dollar(actual[wonnow == "1", sum(AmountUSDnow)])
```


\newpage


## Extra Coefficient for Opportunity Amounts

As seen previously there are lifetime changes in the amount values by 
Opportunity. Which can be seen in dataset head below. `AmountUSD` represent
amount of `Opportinity ID` on "2018-10-01", `AmountUSDnow` represent
"Amount USD" but up to date:


```{r}
head(actual[, .(`Opportunity ID`, `Amount USD`, `AmountUSDnow`)])
```


&nbsp;


As far as we need changes coeffients for the win cases only,
we'll download opportunities of "2018-10-01" from snapshot and filter those IDs
finished as win up to date:


```{r, won_oppos, message=FALSE, cache=TRUE}
nowwd <- getwd()
setwd("../../R_OPPO_PROB")
won <- oppoJoinProbs(given_date = "2018-10-01", ended_as = "won")
setwd(nowwd)
```


&nbsp;


Let's see how many obseravatons (IDs) from "2018-10-01" meets our criteria
(won in the end):


```{r, nrow_won}
nrow(won)
```


&nbsp;


For ease of `won` data exploration I'll narrow dataset keeping variables
we are interested in: `Opportunity ID`, `Amount USD`, `AmountUSDnow` and name
new dataest as `won.cln`. Let's view dataset head:


```{r, won_oppo_cln}
head(won.cln <- won[, .(`Opportunity ID`, `Amount USD`, `AmountUSDnow`)])
```


&nbsp;


Before calculatin difference, let's see investigate initial values
(`Amount USD`) precisely:

```{r, message=FALSE}
require(ggplot2)
qplot(x = won.cln$`Amount USD`, geom = "histogram")
```


&nbsp;


Amount USD values distribution is very strongly right skewed since there are
outlier(s) above 2 Mlns.
Also mode is around zero which requires detailed investigation for errors.
Let's see obseravations where `Amount USD` is less then, say 5000:

```{r}
won.cln[`Amount USD` < 5000]
```


&nbsp;


There are totally `r nrow(won.cln["Amount USD" < 5000])` obseravations where
`Amount USD` is below 5000. We can see above there one value = 1 among them 
which have the same meaning as = 0 so I'll replace all 1 with 0:

```{r}
won.cln[`Amount USD` == 1, `Amount USD` := 0]
won.cln[`AmountUSDnow` == 1, `AmountUSDnow` := 0]
won.cln[`Amount USD` < 5000]
```


&nbsp;


Now we are ready to apply function to calculate the percentage difference by IDs
so we could use these values as a coefficient in distribution of the revenue
simulation. Function Code:


```{r, coef_calc}
diffCoef <- function(begin_num,
                        compl_num,
                        na.rm = FALSE,
                        round = TRUE,
                        na.infinite = TRUE,
                        limit_quant_tails = 0) {
    
    stopifnot(length(begin_num) == length(compl_num))
    stopifnot(is.numeric(begin_num), is.numeric(compl_num))
    
    begin.num <- begin_num
    compl.num <- compl_num
    
    # oppoSQL(given_date = "2018-10-01", ended_as = "all")
    coef0 <- (begin.num == 0L) & (compl.num == 0L)
    coefs <- (begin.num - compl.num) / begin.num
    coefs[coef0] <- 0
    
    if (na.infinite) {
        coefs[is.infinite(coefs)] <- NA
    }
    
    if (round) {
        coefs <- round(coefs, 4)
    }

    if (!is.null(limit_quant_tails) & is.numeric(limit_quant_tails)) {
        stopifnot(limit_quant_tails >= 0 & limit_quant_tails <= 1)
        q.val.left <- quantile(coefs, limit_quant_tails, na.rm = TRUE)
        q.val.right <- quantile(coefs, 1 - limit_quant_tails, na.rm = TRUE)
        message("Limit quantile Values out of ", names(q.val.left), " ",
                names(q.val.right), ": ", q.val.left, " ",  q.val.right)
        coefs[coefs < q.val.left] <- NA
        coefs[coefs > q.val.right] <- NA
    }
    
    if (na.rm) {
        coefs <- coefs[!is.na(coefs)]
        stopifnot(!is.na(coefs))
    }
    
    #what to do with th cases start = 0, end > 0L?
    stopifnot(is.numeric(coefs))
    return(coefs)
}
```


&nbsp;


Coefficient calculation apply `(was - now) / was`
(with 5% outliers cut from both sides), so negative coef is increase by the end
and positive is increase (in percent):


```{r}
won.coef <- won.cln[, diff_coef := diffCoef(begin_num = `Amount USD`,
                                            compl_num = `AmountUSDnow`,
                                            limit_quant_tails = 0.1)]
won.coef <- won.coef[!is.na(diff_coef)]
head(won.coef, 20)
qplot(x = won.coef$diff_coef, geom = "histogram", binwidth = 0.25,
      main = "Coefficients Distribution", xlab = "coeffieinct", ylab = "count")
```


&nbsp;


As we can see most of the values are whether decreased in time or stayed without
changes (mode = 0, nost of the coefs are above 0)

Now let's put these coefficients in a separate variable `coefs`

```{r}
coefs <- won.coef$diff_coef
```


&nbsp;


In order to simulate changes in the amount we would randomly apply them to
those cases that appeared as won in simulation.

```{r}
sim_pp2 <- pvalDist(pvec = actual$`Probability %`,
                   valvec = actual$`Amount USD`,
                   rep = 10000,
                   parallel = TRUE,
                   coefs = coefs)


sim_win_prob2 <- pvalDist(pvec = ful.win.prob,
                         valvec = actual$`Amount USD`,
                         rep = 10000,
                         parallel = TRUE,
                         coefs = coefs)


sim_ml_prob2 <- pvalDist(pvec = join_ml$`Predicted Probability.y`,
                         valvec = join_ml$`Amount USD`,
                         rep = 10000,
                         parallel = TRUE,
                         coefs = coefs)
plotdist(sim_pp2)
plotdist(sim_win_prob2)
plotdist(sim_ml_prob2)

test1 <- data.table(simdist = c(sim_pp, sim_pp2),
                   coef = c(rep(FALSE, length(sim_pp)), rep(TRUE, length(sim_pp2))),
                   Prob_Type = "Probability %")
test2 <- data.table(simdist = c(sim_win_prob, sim_win_prob2),
                   coef = c(rep(FALSE, length(sim_win_prob)), rep(TRUE, length(sim_win_prob2))),
                   Prob_Type = "Winning Probabilities")
test3 <- data.table(simdist = c(sim_ml_prob, sim_ml_prob2),
                   coef = c(rep(FALSE, length(sim_ml_prob)), rep(TRUE, length(sim_ml_prob2))),
                   Prob_Type = "Machine Learning")
test <- rbind(test1,test2,test3)


```


&nbsp;


Recall - actual value is (won opportunities) is
`r round(actual[wonnow == "1", sum(AmountUSDnow)])`

```{r, plot_dens, echo=FALSE}
head(sim_pp2)

act.rev <- actual[wonnow == "1", sum(AmountUSDnow)]
test %>% 
    mutate(coef = coef %>% as.character() %>% forcats::as_factor() %>% forcats::fct_rev()) %>% 
    as_tibble() %>% 
    ggplot(aes(x = simdist, fill = coef)) +
    geom_density(alpha = 0.5, position = "identity", color = "black") +
    scale_x_continuous(labels = dollar_format(scale = 1e-6, suffix = "K"),
                       breaks = scales::pretty_breaks(10)) +
    scale_fill_viridis_d(option = "B") +
    geom_vline(aes(xintercept = act.rev), linetype = 3, size = 2) +
    # geom_text(aes(x = act.rev, y = 0),
    #               label = paste("Actual\n", act.rev), color = "black",
    #               size = 3, angle = 90, vjust = 1.5, hjust = -0.5) +
    expand_limits(0, 0) +
    xlab("Revenue") +
    ylab("Density") +
    labs(title = "Revenue Distribution Simulation",
         subtitle = "probabilities source comparison",
         fill = "Coefficients Applied",
         caption = paste0("Actual Revenue: ", dollar(round(act.rev)))) +
    # theme(legend.position = c(0.9, 0.9)) +
    theme_minimal() +
    theme(legend.position = "right") +
    facet_wrap(~ Prob_Type, ncol = 1)
```


&nbsp;


```{r, plot_dens_coef, echo = FALSE}
test %>%
    filter(coef == TRUE) %>% 
    ggplot(aes(x = simdist, fill = Prob_Type)) +
    geom_density(alpha = 0.5, position = "identity") +
        scale_x_continuous(labels = dollar_format(scale = 1e-6, suffix = "K"),
                       breaks = scales::pretty_breaks(10)) +
    scale_fill_viridis_d(option = "C") +
    geom_vline(aes(xintercept = act.rev), linetype = 3, size = 2) +
    geom_text(aes(x = act.rev, y = 0),
                  label = "Actual Revenue",
              color = "black",
              size = 4, angle = 90, vjust = 1.5, hjust = -0.5) +
    expand_limits(0, 0) +
    xlab("Revenue") +
    ylab("Density") +
    labs(title = "Revenue Distribution Simulation",
         subtitle = "probabilities source comparison",
         fill = "Probability Source",
         caption = paste0("Actual Revenue: ", dollar(round(act.rev)))) +
    # theme(legend.position = c(0.9, 0.9)) +
    theme_minimal() +
    theme(legend.position = "bottom")
```

